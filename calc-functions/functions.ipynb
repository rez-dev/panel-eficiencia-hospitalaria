{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0369035f",
   "metadata": {},
   "source": [
    "# DATOS DE PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8012ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Leer el archivo CSV\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\roesc\\\\Desktop\\\\hospital-efficiency-dashboard_data\\\\df_consolidado_final_v2.csv\")\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame\n",
    "df.head()\n",
    "\n",
    "# pasar valores '--' a 0 en columnas 'Bienes y servicios' y 'Remuneraciones'\n",
    "df['Bienes y servicios'] = df['Bienes y servicios'].replace('--', 0)\n",
    "df['Remuneraciones'] = df['Remuneraciones'].replace('--', 0)\n",
    "\n",
    "# pasar columnas de tipo string a tipo float\n",
    "cols_to_float = ['Bienes y servicios', 'Remuneraciones']\n",
    "for col in cols_to_float:\n",
    "    df[col] = df[col].astype(str).str.replace(',', '.').astype(float)\n",
    "\n",
    "# contar missing en las columnas usando pandas\n",
    "missing_counts = df.isnull().sum()\n",
    "# mostrar los conteos de valores faltantes\n",
    "print(missing_counts)\n",
    "\n",
    "# pasar missing a 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# contar missing en las columnas usando pandas\n",
    "missing_counts = df.isnull().sum()\n",
    "# mostrar los conteos de valores faltantes\n",
    "print(missing_counts)\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_2014 = df_copy[df_copy[\"Año\"] == 2014]\n",
    "df_2016 = df_copy[df_copy[\"Año\"] == 2016]\n",
    "\n",
    "# print shapes\n",
    "print(\"Shape of original DataFrame:\", df.shape)\n",
    "print(\"Shape of 2014 DataFrame:\", df_2014.shape)\n",
    "print(\"Shape of 2016 DataFrame:\", df_2016.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749012c4",
   "metadata": {},
   "source": [
    "# FUNCIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff5e51",
   "metadata": {},
   "source": [
    "## SFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3d2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pysfa import SFA\n",
    "\n",
    "def calculate_sfa_metrics(df: pd.DataFrame,\n",
    "                          input_cols: list[str],\n",
    "                          output_col: list[str],\n",
    "                          te_threshold: float = 0.6,\n",
    "                          fun: str = SFA.FUN_PROD,\n",
    "                          method: str = SFA.TE_teJ) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Ejecuta SFA sobre df y devuelve:\n",
    "      - df_out: df con columna 'Eff_SFA'\n",
    "      - metrics: diccionario con KPI y parámetros clave\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "      Datos con insumos y output.\n",
    "    input_cols : lista de str\n",
    "      Nombres de columnas de insumos.\n",
    "    output_col : str\n",
    "      Nombre de la columna de output.\n",
    "    te_threshold : float\n",
    "      Umbral para definir 'hospital crítico' (TE < te_threshold).\n",
    "    fun : str\n",
    "      Función a usar (SFA.FUN_PROD o FUN_COST).\n",
    "    method : str\n",
    "      Método de eficiencia (SFA.TE_teJ, TE_te, TE_teMod).\n",
    "    \n",
    "    Retorna:\n",
    "    --------\n",
    "    df_out : DataFrame\n",
    "      df con la nueva columna 'Eff_SFA'.\n",
    "    metrics : dict\n",
    "      {\n",
    "        'ET_promedio': float,      # eficiencia técnica promedio\n",
    "        'pct_criticos': float,     # % de CRÍTICOS (TE < te_threshold)\n",
    "        'variable_clave': str,     # insumo con β más alto y p<0.05\n",
    "        'sigma2': float,           # varianza total del error\n",
    "        'betas': np.ndarray,       # todos los β incl. intercept y λ\n",
    "        'p_values': np.ndarray     # todos los p-values\n",
    "      }\n",
    "    \"\"\"\n",
    "    # solo conservar las filas donde los inputs y outputs son mayores que 0\n",
    "    df = df[(df[input_cols] > 0).all(axis=1) & (df[output_col] > 0).all(axis=1)]\n",
    "\n",
    "    x = np.log(df[input_cols]).to_numpy()   # aplicar logaritmo a los inputs\n",
    "    y = np.log(df[output_col]).to_numpy()   # aplicar logaritmo a los outputs\n",
    "\n",
    "    sfa = SFA.SFA(y, x, fun=fun, method=method)\n",
    "    sfa.optimize()\n",
    "    \n",
    "    # Extraer eficiencia y añadirla\n",
    "    te = np.array(sfa.get_technical_efficiency())\n",
    "    df_out = df.copy()\n",
    "    df_out['ET SFA'] = te\n",
    "    \n",
    "    # Extraer parámetros\n",
    "    all_betas = np.array(sfa.get_beta())     # [β0, β1..βk, λ]\n",
    "    all_pvals = np.array(sfa.get_pvalue())\n",
    "    sigma2    = sfa.get_sigma2()\n",
    "    \n",
    "    # ET promedio y % críticos\n",
    "    et_promedio = float(te.mean())\n",
    "    pct_crit    = float((te < te_threshold).mean() * 100)\n",
    "    \n",
    "    # Determinar variable clave\n",
    "    k = len(input_cols)\n",
    "    betas_in = all_betas[1:1+k]\n",
    "    pvals_in = all_pvals[1:1+k]\n",
    "    df_coef = pd.DataFrame({\n",
    "        'input':   input_cols,\n",
    "        'beta':    betas_in,\n",
    "        'p_value': pvals_in\n",
    "    })\n",
    "    df_sign = df_coef[df_coef.p_value < 0.05].copy()\n",
    "    if not df_sign.empty:\n",
    "        df_sign['abs_beta'] = df_sign.beta.abs()\n",
    "        var_clave = df_sign.sort_values('abs_beta', ascending=False).iloc[0].input\n",
    "    else:\n",
    "        var_clave = None\n",
    "    \n",
    "    # Empaquetar métricas\n",
    "    metrics = {\n",
    "        'ET_promedio':    et_promedio,\n",
    "        'pct_criticos':   pct_crit,\n",
    "        'variable_clave': var_clave,\n",
    "        'sigma2':         float(sigma2),\n",
    "        'betas':          all_betas,\n",
    "        'p_values':       all_pvals\n",
    "    }\n",
    "\n",
    "    # imprimir summary de sfa\n",
    "    # print(sfa.summary())\n",
    "    \n",
    "    return df_out, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48692228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sfa, sfa_metrics = calculate_sfa_metrics(\n",
    "    df=df_2014,\n",
    "    input_cols=[\"Bienes y servicios\", \"Remuneraciones\", \"Dias Cama Disponibles\"],\n",
    "    output_col=[\"Quirofanos\"],\n",
    "    te_threshold=0.5\n",
    ")\n",
    "\n",
    "# Ejemplo de lectura de métricas:\n",
    "print(f\"Eficiencia promedio: {sfa_metrics['ET_promedio']:.2%}\")\n",
    "print(f\"% críticos:         {sfa_metrics['pct_criticos']:.2f}%\")\n",
    "print(f\"Variable clave:     {sfa_metrics['variable_clave']}\")\n",
    "print(f\"Varianza σ²:        {sfa_metrics['sigma2']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a21bd0d",
   "metadata": {},
   "source": [
    "## DEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b651283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Pyfrontier.frontier_model import EnvelopDEA\n",
    "\n",
    "def calculate_dea_metrics(df: pd.DataFrame,\n",
    "                          input_cols: list[str],\n",
    "                          output_cols: list[str],\n",
    "                          orientation: str = \"in\",\n",
    "                          rts: str = \"CRS\",\n",
    "                          te_threshold: float = 0.6,\n",
    "                          n_jobs: int = 1\n",
    "                         ) -> tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Ejecuta un DEA y devuelve:\n",
    "      - df_out: df con columna 'Eff_DEA'\n",
    "      - metrics: {\n",
    "          'ET_promedio': float,\n",
    "          'pct_criticos': float,\n",
    "          'variable_slack_clave': str,\n",
    "          'rts': str,\n",
    "          'orientation': str\n",
    "        }\n",
    "    \n",
    "    Parámetros:\n",
    "      df           : DataFrame con tus datos\n",
    "      input_cols   : lista de nombres de columnas de insumos\n",
    "      output_cols  : lista de nombres de columnas de outputs\n",
    "      orientation  : 'in' o 'out'\n",
    "      rts          : 'CRS' o 'VRS'\n",
    "      te_threshold : umbral para % críticos (score < te_threshold)\n",
    "    \"\"\"\n",
    "    df = df[(df[input_cols] > 0).all(axis=1) & (df[output_cols] > 0).all(axis=1)]\n",
    "\n",
    "    # 1) Armar arrays\n",
    "    x = df[input_cols].to_numpy()\n",
    "    y = df[output_cols].to_numpy()\n",
    "    \n",
    "    # 2) Entrenar DEA\n",
    "    dea_crs = EnvelopDEA(rts, orientation, n_jobs=n_jobs)\n",
    "    dea_crs.fit(x, y)\n",
    "    \n",
    "    # 3) Scores y slacks\n",
    "    scores_crs = np.array([res.score for res in dea_crs.result])\n",
    "    # slacks: entrada [n_inputs x_slack_i] en res.x_slack\n",
    "    slacks_crs = np.stack([res.x_slack for res in dea_crs.result], axis=0)  # shape (n, k)\n",
    "    \n",
    "    # 4) DataFrame de salida\n",
    "    df_out = df.copy()\n",
    "    df_out[\"ET DEA\"] = scores_crs\n",
    "    \n",
    "    # 5) KPI: promedio y críticos\n",
    "    et_promedio = float(scores_crs.mean())\n",
    "    pct_crit    = float((scores_crs < te_threshold).mean() * 100)\n",
    "    \n",
    "    # 6) Variable slack clave: el input cuyo slack medio es mayor\n",
    "    mean_slacks = np.nanmean(np.where(slacks_crs==0, np.nan, slacks_crs), axis=0)\n",
    "    idx_max     = int(np.nanargmax(mean_slacks))\n",
    "    var_slack_clave = input_cols[idx_max]\n",
    "    \n",
    "    # 8) Empaquetar métricas\n",
    "    metrics = {\n",
    "        \"ET_promedio\":          et_promedio,\n",
    "        \"pct_criticos\":         pct_crit,\n",
    "        \"variable_slack_clave\": var_slack_clave,\n",
    "        \"orientation\":          orientation,\n",
    "        \"rts\":                  rts\n",
    "    }\n",
    "    return df_out, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dea, dea_metrics = calculate_dea_metrics(\n",
    "    df=df_2014,\n",
    "    input_cols=[\"Bienes y servicios\", \"Remuneraciones\", \"Dias Cama Disponibles\"],\n",
    "    output_cols=[\"Consultas\"],\n",
    "    orientation=\"in\",\n",
    "    rts=\"CRS\",\n",
    "    te_threshold=0.6,\n",
    "    n_jobs=2\n",
    ")\n",
    "\n",
    "print(f\"ET Promedio:      {dea_metrics['ET_promedio']:.2%}\")\n",
    "print(f\"% Hosp. críticos:  {dea_metrics['pct_criticos']:.2f}%\")\n",
    "print(f\"Variable slack clave: {dea_metrics['variable_slack_clave']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1ec19e",
   "metadata": {},
   "source": [
    "## DEA-M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27468b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Pyfrontier.frontier_model import EnvelopDEA\n",
    "\n",
    "def calculate_dea_malmquist_simple(df_t, df_t1,\n",
    "                                   input_cols, output_cols,\n",
    "                                   rts=\"CRS\", orientation=\"in\",\n",
    "                                   n_jobs: int = 1):\n",
    "    \"\"\"\n",
    "    Índice de Malmquist simplificado = solo cambio de eficiencia\n",
    "    (sin re-resolver con cross-eficiencias).\n",
    "    \n",
    "    Devuelve:\n",
    "      • df_out: EFF_t, EFF_t1, EFFCH (EFF_t1/EFF_t)\n",
    "      • summary: media de EFFCH\n",
    "    \"\"\"\n",
    "    # 1) Índice común de hospitales\n",
    "    idx = df_t.index.intersection(df_t1.index)\n",
    "    df_t  = df_t.loc[idx]\n",
    "    df_t1 = df_t1.loc[idx]\n",
    "\n",
    "    # 2) Calcular EFF en cada periodo con el helper DEA\n",
    "    def dea_scores(df_period):\n",
    "        X = df_period[input_cols].to_numpy(dtype=float)\n",
    "        Y = df_period[output_cols].to_numpy(dtype=float)\n",
    "        dea = EnvelopDEA(rts, orientation, n_jobs=n_jobs)\n",
    "        dea.fit(X, Y)\n",
    "        return np.array([res.score for res in dea.result])\n",
    "\n",
    "    eff_t  = dea_scores(df_t)\n",
    "    eff_t1 = dea_scores(df_t1)\n",
    "\n",
    "    # 3) Cambio de eficiencia (EFFCH)\n",
    "    effch = eff_t1 / eff_t\n",
    "\n",
    "    # 4) DataFrame de salida\n",
    "    df_out = pd.DataFrame({\n",
    "        \"EFF_t\":   eff_t,\n",
    "        \"EFF_t1\":  eff_t1,\n",
    "        \"EFFCH\":   effch\n",
    "    }, index=idx)\n",
    "\n",
    "    # 5) Resumen\n",
    "    summary = {\n",
    "        \"EFFCH_mean\": float(effch.mean())\n",
    "    }\n",
    "    return df_out, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9f5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malm_simp, malm_simp_meta = calculate_dea_malmquist_simple(\n",
    "    df_2014, df_2016,\n",
    "    input_cols  = [\"Bienes y servicios\", \"Remuneraciones\", \"Dias Cama Disponibles\"],\n",
    "    output_cols = [\"Consultas\"],\n",
    "    rts=\"CRS\", orientation=\"in\",\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "print(\"Cambio de eficiencia promedio (EFFCH):\",\n",
    "      f\"{malm_simp_meta['EFFCH_mean']:.3f}\")\n",
    "df_malm_simp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8263f51b",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd1760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def run_pca(df: pd.DataFrame,\n",
    "            feature_cols: List[str],\n",
    "            n_components: int | None = None,\n",
    "            scale: bool = True\n",
    "           ) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Ejecuta PCA sobre las columnas `feature_cols` y devuelve:\n",
    "      • df_pca  : DataFrame con columnas PC1, PC2, …, PCk\n",
    "      • metrics : dict con varianza explicada y matriz de cargas\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    df           : DataFrame original\n",
    "    feature_cols : columnas a incluir en el PCA (solo numéricas)\n",
    "    n_components : nº de componentes (None ⇒ tantas como variables)\n",
    "    scale        : estandarizar variables a media 0 y σ 1 antes de PCA\n",
    "    \n",
    "    Ejemplo\n",
    "    -------\n",
    "    df_pca, meta = run_pca(df, [\"slack_bs\", \"slack_rem\", \"slack_cama\"], 2)\n",
    "    \"\"\"\n",
    "    X = df[feature_cols].to_numpy(dtype=float)\n",
    "    \n",
    "    # Escalado opcional\n",
    "    if scale:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    pca = PCA(n_components=n_components)\n",
    "    pcs = pca.fit_transform(X)\n",
    "    \n",
    "    pc_cols = [f\"PC{i+1}\" for i in range(pcs.shape[1])]\n",
    "    df_pca  = pd.DataFrame(pcs, index=df.index, columns=pc_cols)\n",
    "    \n",
    "    metrics = {\n",
    "        \"explained_variance_ratio\": pca.explained_variance_ratio_.tolist(),\n",
    "        \"components\": pd.DataFrame(pca.components_,\n",
    "                                   index=pc_cols,\n",
    "                                   columns=feature_cols)\n",
    "    }\n",
    "    return df_pca, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f3d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Selecciona las columnas que quieres analizar\n",
    "input_cols  = [\"Bienes y servicios\", \"Remuneraciones\"]\n",
    "output_cols = [\"Consultas\", \"Quirofanos\"]\n",
    "\n",
    "# unir columnas en una sola lista\n",
    "cols_pca = input_cols + output_cols\n",
    "\n",
    "# 2) Ejecuta PCA (2 componentes para plot)\n",
    "df_pca, pca_meta = run_pca(df_2014, cols_pca, n_components=2)\n",
    "\n",
    "# 3) KPI rápidos\n",
    "print(\"Varianza explicada:\", pca_meta[\"explained_variance_ratio\"])\n",
    "print(\"\\nMatriz de cargas:\")\n",
    "print(pca_meta[\"components\"])\n",
    "\n",
    "loadings = pca_meta[\"components\"]   # DataFrame PC×vars\n",
    "squared = loadings**2           # coef²\n",
    "contribs = squared.div(squared.sum(axis=1), axis=0)\n",
    "\n",
    "# 4) Mostrar contribuciones de cada variable a los PCs\n",
    "print(\"\\nContribuciones de variables a los PCs:\")\n",
    "print(contribs)\n",
    "\n",
    "# 4) Ejemplo de visualización 2-D\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(df_pca[\"PC1\"], df_pca[\"PC2\"])\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Hosp. en espacio de Componentes Principales\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9769cbf",
   "metadata": {},
   "source": [
    "## CLUSTERIZACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f33738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ------- usa la función run_pca que ya tienes --------\n",
    "def pca_kmeans(df: pd.DataFrame,\n",
    "               feature_cols: List[str],\n",
    "               n_components: int = 2,\n",
    "               k: int | None = None,\n",
    "               k_max: int = 10,\n",
    "               scale: bool = True,\n",
    "               random_state: int = 42\n",
    "              ) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    1) Ejecuta PCA con run_pca\n",
    "    2) Aplica K-means (elige k óptimo con silhouette si k=None)\n",
    "    3) Devuelve df con PCs y 'cluster', y un diccionario de metadatos\n",
    "    \"\"\"\n",
    "    # ---- PCA --------------------------------------------------\n",
    "    df_pca, pca_meta = run_pca(df, feature_cols,\n",
    "                               n_components=n_components,\n",
    "                               scale=scale)\n",
    "    pc_cols = df_pca.columns.tolist()\n",
    "\n",
    "    # ---- elegir k si no viene fijo ---------------------------\n",
    "    if k is None:\n",
    "        best_k, best_score = 2, -1\n",
    "        for kk in range(2, k_max + 1):\n",
    "            km = KMeans(n_clusters=kk, n_init=\"auto\", random_state=random_state)\n",
    "            labels = km.fit_predict(df_pca[pc_cols])\n",
    "            score = silhouette_score(df_pca[pc_cols], labels)\n",
    "            if score > best_score:\n",
    "                best_k, best_score = kk, score\n",
    "        k = best_k\n",
    "        silhouette_best = best_score\n",
    "    else:\n",
    "        silhouette_best = None\n",
    "\n",
    "    # ---- K-means definitivo -----------------------------------\n",
    "    kmeans = KMeans(n_clusters=k, n_init=\"auto\", random_state=random_state)\n",
    "    cluster_labels = kmeans.fit_predict(df_pca[pc_cols])\n",
    "\n",
    "    # ---- ensamblar DataFrame de salida ------------------------\n",
    "    df_out = df.copy()\n",
    "    df_out = pd.concat([df_out, df_pca], axis=1)\n",
    "    df_out[\"cluster\"] = cluster_labels\n",
    "\n",
    "    # ---- empaquetar metadatos ---------------------------------\n",
    "    meta = {\n",
    "        \"explained_variance_ratio\": pca_meta[\"explained_variance_ratio\"],\n",
    "        \"components\": pca_meta[\"components\"],\n",
    "        \"k\": k,\n",
    "        \"silhouette\": silhouette_best,\n",
    "        \"cluster_centers\": pd.DataFrame(kmeans.cluster_centers_,\n",
    "                                        columns=pc_cols)\n",
    "    }\n",
    "    return df_out, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columnas para el PCA\n",
    "feature_cols = [\"Bienes y servicios\", \"Remuneraciones\",\n",
    "                \"Dias Cama Disponibles\",\"ET SFA\"]\n",
    "\n",
    "# ejecutar sfa con funcion creada\n",
    "df_sfa, sfa_metrics = calculate_sfa_metrics(\n",
    "    df=df_2014,\n",
    "    input_cols=[\"Bienes y servicios\", \"Remuneraciones\", \"Dias Cama Disponibles\"],\n",
    "    output_col=[\"Quirofanos\"],\n",
    "    te_threshold=0.5\n",
    ")\n",
    "\n",
    "df_cluster, info = pca_kmeans(df_sfa,\n",
    "                              feature_cols=feature_cols,\n",
    "                              n_components=3,   # sólo PC1 y PC2\n",
    "                              k=None,           # deja que la función elija\n",
    "                              k_max=8)\n",
    "\n",
    "print(\"k elegido:\", info[\"k\"])\n",
    "print(\"Varianza explicada:\", info[\"explained_variance_ratio\"])\n",
    "print(\"\\nMatriz de cargas:\")\n",
    "print(info[\"components\"])\n",
    "\n",
    "# 1) Agrupa y agrega\n",
    "summary = df_cluster.groupby(\"cluster\").agg(\n",
    "    N_Hospitales = (\"cluster\", \"size\"),\n",
    "    TE_Media     = (\"ET SFA\", \"mean\"),\n",
    "    PC1_media    = (\"PC1\", \"mean\"),\n",
    "    PC2_media    = (\"PC2\", \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "# 2) Formatea un poquito (opcional)\n",
    "summary[\"TE_Media\"]  = summary[\"TE_Media\"].round(2)\n",
    "summary[\"PC1_media\"] = summary[\"PC1_media\"].round(2)\n",
    "summary[\"PC2_media\"] = summary[\"PC2_media\"].round(2)\n",
    "\n",
    "# 3) Muestra\n",
    "print(\"\\nResumen de clusters:\")\n",
    "print(summary)\n",
    "\n",
    "# Visualizar\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "for cl in range(info[\"k\"]):\n",
    "    mask = df_cluster[\"cluster\"] == cl\n",
    "    plt.scatter(df_cluster.loc[mask, \"PC1\"],\n",
    "                df_cluster.loc[mask, \"PC2\"],\n",
    "                s=25, label=f\"Cluster {cl}\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.legend()\n",
    "plt.title(\"Hosp. agrupados por K-means en espacio PCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63234eb6",
   "metadata": {},
   "source": [
    "## ANÁLISIS DE DETERMINANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e70322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deter_analysis.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "def determinant_analysis(df: pd.DataFrame,\n",
    "                         dependent: str,\n",
    "                         independents: List[str],\n",
    "                         top_n: int = 5,\n",
    "                         add_constant: bool = True\n",
    "                        ) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Ajusta un modelo OLS  y entrega la tabla de coeficientes + métricas resumidas.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    df          : DataFrame con los datos (sin nulos en cols usadas)\n",
    "    dependent   : nombre de la columna dependiente (Y)\n",
    "    independents: lista de columnas explicativas (X)\n",
    "    top_n       : nº de determinantes \"clave\" a destacar\n",
    "    add_constant: añade intercepto si True\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    coef_table : DataFrame con coef, std_err, t, p\n",
    "    meta       : dict  {\n",
    "                         'r2'           : float,\n",
    "                         'r2_adj'       : float,\n",
    "                         'top_vars'     : list[str]\n",
    "                       }\n",
    "    \"\"\"\n",
    "    # 0) Filtrar filas completas\n",
    "    df_clean = df.dropna(subset=[dependent] + independents).copy()\n",
    "\n",
    "    # 1) Matrices\n",
    "    y = df_clean[dependent].astype(float).to_numpy()\n",
    "    X = df_clean[independents].astype(float)\n",
    "    if add_constant:\n",
    "        X = sm.add_constant(X)\n",
    "    \n",
    "    # 2) Ajustar modelo\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # 3) Tabla de coeficientes\n",
    "    coef_table = model.summary2().tables[1]                 # coef, std err, t, P>|t|\n",
    "    coef_table.index.name = \"variable\"\n",
    "    coef_table.reset_index(inplace=True)\n",
    "    \n",
    "    # 4) Variables “clave”  (|coef| grande & p<0.05)\n",
    "    sig = coef_table[coef_table[\"P>|t|\"] < 0.05].copy()\n",
    "    sig[\"abs_coef\"] = sig[\"Coef.\"].abs()\n",
    "    sig_sorted = sig.sort_values(\"abs_coef\", ascending=False)\n",
    "    top_vars = sig_sorted[\"variable\"].head(top_n).tolist()\n",
    "    \n",
    "    # 5) Métricas de resumen\n",
    "    meta = {\n",
    "        \"r2\":      model.rsquared,\n",
    "        \"r2_adj\":  model.rsquared_adj,\n",
    "        \"top_vars\": top_vars\n",
    "    }\n",
    "    return coef_table, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8727fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_tbl, info = determinant_analysis(\n",
    "    df           = df_sample,\n",
    "    dependent    = \"Eff_DEA\",              # o el KPI que quieras explicar\n",
    "    independents = [\"Bienes y servicios\",\n",
    "                    \"Remuneraciones\",\n",
    "                    \"Días Cama Disponibles\",\n",
    "                    \"Consultas\"],\n",
    "    top_n        = 3\n",
    ")\n",
    "\n",
    "print(\"R²:\", info[\"r2\"]:.3f, \"  –  R² ajustado:\", info[\"r2_adj\"]:.3f)\n",
    "print(\"Determinantes clave:\", info[\"top_vars\"])\n",
    "display(coef_tbl)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
